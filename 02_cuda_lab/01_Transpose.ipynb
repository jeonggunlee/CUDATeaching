{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonggunlee/CUDATeaching/blob/master/02_cuda_lab/01_Transpose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7UT_qN20klON"
      },
      "source": [
        "## Matrix Transpose Optimization\n",
        "\n",
        "![Matrix Transpose](https://2.bp.blogspot.com/-8LzbJv0zB3A/WAzRQbxP5eI/AAAAAAAAHYU/MEqPV8JxtLMCSGSQ-0UKZSYlUN3jALZaQCLcB/s1600/Java%2BProgram%2Bto%2BTranspose%2Ba%2BMatrix%2B.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "LJLKazMijoQF",
        "outputId": "53fb8254-5dd7-43a9-91b7-fd8310a17951"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing copy.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile copy.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// simple copy kernel\n",
        "// Used as reference case representing best effective bandwidth.\n",
        "__global__ void copy(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[(y+j)*width + x] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// copy kernel using shared memory\n",
        "// Also used as reference case, demonstrating effect of using shared memory.\n",
        "__global__ void copySharedMem(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM * TILE_DIM];\n",
        "  \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x];          \n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "  \n",
        "  // ----\n",
        "  // copy \n",
        "  // ----\n",
        "  printf(\"%25s\", \"copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copy<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx*ny, ms);\n",
        "\n",
        "  // -------------\n",
        "  // copySharedMem \n",
        "  // -------------\n",
        "  printf(\"%25s\", \"shared memory copy\");\n",
        "  checkCuda( cudaMemset(d_cdata, 0, mem_size) );\n",
        "  // warm up\n",
        "  copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     copySharedMem<<<dimGrid, dimBlock>>>(d_cdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_cdata, d_cdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(h_idata, h_cdata, nx * ny, ms);\n",
        "\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "g7E6xCy9kEM1"
      },
      "outputs": [],
      "source": [
        "!nvcc -o copy copy.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "colab_type": "code",
        "id": "g7STxyl7kKRg",
        "outputId": "f83b927c-d6c6-47e0-bc14-5955d2a7958c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Device : NVIDIA Graphics Device\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "                     copy              819.20\n",
            "       shared memory copy              909.21\n"
          ]
        }
      ],
      "source": [
        "!./copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "colab_type": "code",
        "id": "5BubEgUIk3YH",
        "outputId": "c73bb687-d4c4-420e-a8b7-c6b693bc2e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.\n",
            "                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.\n",
            "                  Refer https://developer.nvidia.com/tools-overview for more details.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvprof ./copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "a5I_sbPLh_kn",
        "outputId": "5aeb7bc8-9bb7-4613-f0b0-06168247ddb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing transpose.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile transpose.cu\n",
        "\n",
        "/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Convenience function for checking CUDA runtime API results\n",
        "// can be wrapped around any runtime API call. No-op in release builds.\n",
        "inline\n",
        "cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "#if defined(DEBUG) || defined(_DEBUG)\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "#endif\n",
        "  return result;\n",
        "}\n",
        "\n",
        "const int TILE_DIM = 32;\n",
        "const int BLOCK_ROWS = 8;\n",
        "const int NUM_REPS = 100;\n",
        "\n",
        "// Check errors and print GB/s\n",
        "void postprocess(const float *ref, const float *res, int n, float ms)\n",
        "{\n",
        "  bool passed = true;\n",
        "  for (int i = 0; i < n; i++)\n",
        "    if (res[i] != ref[i]) {\n",
        "      printf(\"%d %f %f\\n\", i, res[i], ref[i]);\n",
        "      printf(\"%25s\\n\", \"*** FAILED ***\");\n",
        "      passed = false;\n",
        "      break;\n",
        "    }\n",
        "  if (passed)\n",
        "    printf(\"%20.2f\\n\", 2 * n * sizeof(float) * 1e-6 * NUM_REPS / ms );\n",
        "}\n",
        "\n",
        "// naive transpose\n",
        "// Simplest transpose; doesn't use shared memory.\n",
        "// Global memory reads are coalesced but writes are not.\n",
        "__global__ void transposeNaive(float *odata, const float *idata)\n",
        "{\n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)\n",
        "    odata[x*width + (y+j)] = idata[(y+j)*width + x];\n",
        "}\n",
        "\n",
        "// coalesced transpose\n",
        "// Uses shared memory to achieve coalesing in both reads and writes\n",
        "// Tile width == #banks causes shared memory bank conflicts.\n",
        "__global__ void transposeCoalesced(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "   \n",
        "\n",
        "// No bank-conflict transpose\n",
        "// Same as transposeCoalesced except the first tile dimension is padded \n",
        "// to avoid shared memory bank conflicts.\n",
        "__global__ void transposeNoBankConflicts(float *odata, const float *idata)\n",
        "{\n",
        "  __shared__ float tile[TILE_DIM][TILE_DIM+1];\n",
        "    \n",
        "  int x = blockIdx.x * TILE_DIM + threadIdx.x;\n",
        "  int y = blockIdx.y * TILE_DIM + threadIdx.y;\n",
        "  int width = gridDim.x * TILE_DIM;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];\n",
        "\n",
        "  __syncthreads();\n",
        "\n",
        "  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset\n",
        "  y = blockIdx.x * TILE_DIM + threadIdx.y;\n",
        "\n",
        "  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)\n",
        "     odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv)\n",
        "{\n",
        "  const int nx = 1024;\n",
        "  const int ny = 1024;\n",
        "  const int mem_size = nx*ny*sizeof(float);\n",
        "\n",
        "  dim3 dimGrid(nx/TILE_DIM, ny/TILE_DIM, 1);\n",
        "  dim3 dimBlock(TILE_DIM, BLOCK_ROWS, 1);\n",
        "\n",
        "  int devId = 0;\n",
        "  if (argc > 1) devId = atoi(argv[1]);\n",
        "\n",
        "  cudaDeviceProp prop;\n",
        "  checkCuda( cudaGetDeviceProperties(&prop, devId));\n",
        "  printf(\"\\nDevice : %s\\n\", prop.name);\n",
        "  printf(\"Matrix size: %d %d, Block size: %d %d, Tile size: %d %d\\n\", \n",
        "         nx, ny, TILE_DIM, BLOCK_ROWS, TILE_DIM, TILE_DIM);\n",
        "  printf(\"dimGrid: %d %d %d. dimBlock: %d %d %d\\n\",\n",
        "         dimGrid.x, dimGrid.y, dimGrid.z, dimBlock.x, dimBlock.y, dimBlock.z);\n",
        "  \n",
        "  checkCuda( cudaSetDevice(devId) );\n",
        "\n",
        "  float *h_idata = (float*)malloc(mem_size);\n",
        "  float *h_cdata = (float*)malloc(mem_size);\n",
        "  float *h_tdata = (float*)malloc(mem_size);\n",
        "  float *gold    = (float*)malloc(mem_size);\n",
        "  \n",
        "  float *d_idata, *d_cdata, *d_tdata;\n",
        "  checkCuda( cudaMalloc(&d_idata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_cdata, mem_size) );\n",
        "  checkCuda( cudaMalloc(&d_tdata, mem_size) );\n",
        "\n",
        "  // check parameters and calculate execution configuration\n",
        "  if (nx % TILE_DIM || ny % TILE_DIM) {\n",
        "    printf(\"nx and ny must be a multiple of TILE_DIM\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "\n",
        "  if (TILE_DIM % BLOCK_ROWS) {\n",
        "    printf(\"TILE_DIM must be a multiple of BLOCK_ROWS\\n\");\n",
        "    goto error_exit;\n",
        "  }\n",
        "    \n",
        "  // host\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      h_idata[j*nx + i] = j*nx + i;\n",
        "\n",
        "  // correct result for error checking\n",
        "  for (int j = 0; j < ny; j++)\n",
        "    for (int i = 0; i < nx; i++)\n",
        "      gold[j*nx + i] = h_idata[i*nx + j];\n",
        "  \n",
        "  // device\n",
        "  checkCuda( cudaMemcpy(d_idata, h_idata, mem_size, cudaMemcpyHostToDevice) );\n",
        "  \n",
        "  // events for timing\n",
        "  cudaEvent_t startEvent, stopEvent;\n",
        "  checkCuda( cudaEventCreate(&startEvent) );\n",
        "  checkCuda( cudaEventCreate(&stopEvent) );\n",
        "  float ms;\n",
        "\n",
        "  // ------------\n",
        "  // time kernels\n",
        "  // ------------\n",
        "  printf(\"%25s%25s\\n\", \"Routine\", \"Bandwidth (GB/s)\");\n",
        "\n",
        "  // --------------\n",
        "  // transposeNaive \n",
        "  // --------------\n",
        "  printf(\"%25s\", \"naive transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNaive<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------\n",
        "  // transposeCoalesced \n",
        "  // ------------------\n",
        "  printf(\"%25s\", \"coalesced transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeCoalesced<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "  // ------------------------\n",
        "  // transposeNoBankConflicts\n",
        "  // ------------------------\n",
        "  printf(\"%25s\", \"conflict-free transpose\");\n",
        "  checkCuda( cudaMemset(d_tdata, 0, mem_size) );\n",
        "  // warmup\n",
        "  transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(startEvent, 0) );\n",
        "  for (int i = 0; i < NUM_REPS; i++)\n",
        "     transposeNoBankConflicts<<<dimGrid, dimBlock>>>(d_tdata, d_idata);\n",
        "  checkCuda( cudaEventRecord(stopEvent, 0) );\n",
        "  checkCuda( cudaEventSynchronize(stopEvent) );\n",
        "  checkCuda( cudaEventElapsedTime(&ms, startEvent, stopEvent) );\n",
        "  checkCuda( cudaMemcpy(h_tdata, d_tdata, mem_size, cudaMemcpyDeviceToHost) );\n",
        "  postprocess(gold, h_tdata, nx * ny, ms);\n",
        "\n",
        "error_exit:\n",
        "  // cleanup\n",
        "  checkCuda( cudaEventDestroy(startEvent) );\n",
        "  checkCuda( cudaEventDestroy(stopEvent) );\n",
        "  checkCuda( cudaFree(d_tdata) );\n",
        "  checkCuda( cudaFree(d_cdata) );\n",
        "  checkCuda( cudaFree(d_idata) );\n",
        "  free(h_idata);\n",
        "  free(h_tdata);\n",
        "  free(h_cdata);\n",
        "  free(gold);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "colab_type": "code",
        "id": "C4qZUeGJiyX5",
        "outputId": "e055e10a-698d-4a12-d722-d4b1b0375afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "00_googleDrive_CUDAExam.ipynb\t  atomicAdd.cu\tREADME.md\n",
            "00_UnifiedMemory_SharedMem.ipynb  clock.cu\treduction_all.cu\n",
            "01_Transpose.ipynb\t\t  copy\t\ttranspose.cu\n",
            "03_reduction.ipynb\t\t  copy.cu\n",
            "04_atomic.ipynb\t\t\t  gpu_timer.h\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cPqutafhi1Q4"
      },
      "outputs": [],
      "source": [
        "!nvcc -o transpose transpose.cu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "colab_type": "code",
        "id": "8MX15rCYjKg3",
        "outputId": "98484f08-3bc9-4fed-b358-79ddcca9ae46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Device : NVIDIA Graphics Device\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "          naive transpose              268.33\n",
            "      coalesced transpose              673.13\n",
            "  conflict-free transpose              914.29\n"
          ]
        }
      ],
      "source": [
        "!./transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "colab_type": "code",
        "id": "Vizv5p5ck9Pv",
        "outputId": "029590d9-46b9-435c-951f-acfc187be2cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: transpose and any of its children processes will be profiled.\n",
            "\n",
            "\n",
            "Device : NVIDIA Graphics Device\n",
            "Matrix size: 1024 1024, Block size: 32 8, Tile size: 32 32\n",
            "dimGrid: 32 32 1. dimBlock: 32 8 1\n",
            "                  Routine         Bandwidth (GB/s)\n",
            "          naive transpose              259.52\n",
            "      coalesced transpose              620.80\n",
            "  conflict-free transpose              821.61\n",
            "Generating '/tmp/nsys-report-7af5.qdstrm'\n",
            "[1/7] [========================100%] report2.nsys-rep\n",
            "[2/7] [========================100%] report1.sqlite\n",
            "[3/7] Executing 'nvtxsum' stats report\n",
            "SKIPPED: /home/eulia/CUDA/CUDATeaching/02_cuda_lab/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/7] Executing 'cudaapisum' stats report\n",
            "\n",
            "CUDA API Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)           Name        \n",
            " --------  ---------------  ---------  ------------  -----------  --------  ----------  ------------  --------------------\n",
            "     89.7       68,779,221          3  22,926,407.0     23,045.0    19,033  68,737,143  39,673,261.2  cudaMalloc          \n",
            "      6.8        5,250,312          3   1,750,104.0  1,217,894.0   884,036   3,148,382   1,222,395.7  cudaEventSynchronize\n",
            "      2.5        1,903,901          4     475,975.3    329,513.0   178,347   1,066,528     400,103.9  cudaMemcpy          \n",
            "      0.6          424,809        303       1,402.0      1,330.0     1,226       7,676         413.5  cudaLaunchKernel    \n",
            "      0.3          258,114          3      86,038.0    106,496.0    43,490     108,128      36,856.7  cudaFree            \n",
            "      0.0           21,996          3       7,332.0      4,436.0     3,470      14,090       5,872.5  cudaMemset          \n",
            "      0.0            7,714          6       1,285.7        987.5       836       2,679         703.7  cudaEventRecord     \n",
            "      0.0            5,325          2       2,662.5      2,662.5       594       4,731       2,925.3  cudaEventCreate     \n",
            "      0.0            1,267          2         633.5        633.5       217       1,050         589.0  cudaEventDestroy    \n",
            "\n",
            "[5/7] Executing 'gpukernsum' stats report\n",
            "\n",
            "CUDA Kernel Statistics:\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)                        Name                      \n",
            " --------  ---------------  ---------  --------  --------  --------  --------  -----------  ------------------------------------------------\n",
            "     58.6        3,194,704        101  31,630.7  31,744.0    30,976    32,129        319.9  transposeNaive(float *, const float *)          \n",
            "     23.8        1,294,945        101  12,821.2  12,864.0    12,192    13,504        268.4  transposeCoalesced(float *, const float *)      \n",
            "     17.6          960,489        101   9,509.8   9,472.0     9,184     9,984        138.0  transposeNoBankConflicts(float *, const float *)\n",
            "\n",
            "[6/7] Executing 'gpumemtimesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by time):\n",
            "\n",
            " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  ---------  ---------  --------  --------  -----------  ------------------\n",
            "     83.7        1,067,621      3  355,873.7  236,001.0   232,481   599,139    210,681.3  [CUDA memcpy DtoH]\n",
            "     15.2          193,889      1  193,889.0  193,889.0   193,889   193,889          0.0  [CUDA memcpy HtoD]\n",
            "      1.1           14,081      3    4,693.7    4,001.0     3,840     6,240      1,341.6  [CUDA memset]     \n",
            "\n",
            "[7/7] Executing 'gpumemsizesum' stats report\n",
            "\n",
            "CUDA Memory Operation Statistics (by size):\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     12.583      3     4.194     4.194     4.194     4.194        0.000  [CUDA memcpy DtoH]\n",
            "     12.583      3     4.194     4.194     4.194     4.194        0.000  [CUDA memset]     \n",
            "      4.194      1     4.194     4.194     4.194     4.194        0.000  [CUDA memcpy HtoD]\n",
            "\n",
            "Generated:\n",
            "    /home/eulia/CUDA/CUDATeaching/02_cuda_lab/report2.nsys-rep\n",
            "    /home/eulia/CUDA/CUDATeaching/02_cuda_lab/report1.sqlite\n"
          ]
        }
      ],
      "source": [
        "!nsys nvprof ./transpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CJ8YLJJTjM3G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "01_Transpose.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('pytorch_1.12': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "b9a2f748aac4447c26e061a4a5eb6fae93defb5f5907b82649bbfaeaf0a4d8c8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
